"""Forecasting logic for research planning and source classification."""

import json
import re
from typing import TypedDict
from urllib.parse import urlparse

from config import Config, load_config
from llm_client import LLMClient, LLMClientError
from search_engine import perform_search


class SearchPlan(TypedDict):
    """Search plan generated by the LLM."""
    search_queries: list[str]
    historical_queries: list[str]
    time_window: str


# Module-level cache for domain tier classifications
_domain_tier_cache: dict[str, int] = {}


async def generate_search_plan(
    client: LLMClient,
    question: str,
    user_ref_class: str | None = None,
) -> SearchPlan:
    """
    Generate a search plan for a forecasting question.
    
    Uses the LLM to create search queries for current news and
    historical base rates.
    
    Args:
        client: Initialized LLMClient instance.
        question: The forecasting question to research.
        user_ref_class: Optional reference class to guide historical queries
                        (e.g., "tech IPOs", "geopolitical conflicts").
    
    Returns:
        A SearchPlan dictionary with search_queries, historical_queries,
        and time_window.
    """
    ref_class_instruction = ""
    if user_ref_class:
        ref_class_instruction = (
            f"\nThe user has suggested a reference class: '{user_ref_class}'. "
            "Use this to guide historical/base rate queries."
        )
    
    prompt = f"""You are a research assistant for forecasting. Given a question, generate a search plan.

Question: {question}
{ref_class_instruction}

Output a JSON object with exactly these fields:
- "search_queries": A list of 3-5 search queries for finding current news and developments.
- "historical_queries": A list of 2-3 search queries for finding base rates and historical precedents.
- "time_window": A suggested lookback period (e.g., "6m", "1y", "2y").

Return ONLY the JSON object, no other text."""

    messages = [{"role": "user", "content": prompt}]
    
    try:
        response = await client.generate_response(messages, temperature=0.3)
        
        # Extract JSON from response (handle markdown code blocks)
        json_match = re.search(r"```(?:json)?\s*(.*?)```", response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1).strip()
        else:
            json_str = response.strip()
        
        parsed = json.loads(json_str)
        
        return SearchPlan(
            search_queries=parsed.get("search_queries", []),
            historical_queries=parsed.get("historical_queries", []),
            time_window=parsed.get("time_window", "6m"),
        )
        
    except (json.JSONDecodeError, LLMClientError) as e:
        # Return a minimal fallback plan
        return SearchPlan(
            search_queries=[question],
            historical_queries=[],
            time_window="6m",
        )


def _extract_domain(url: str) -> str:
    """Extract the base domain from a URL."""
    parsed = urlparse(url)
    domain = parsed.netloc or parsed.path
    # Remove www. prefix
    if domain.startswith("www."):
        domain = domain[4:]
    return domain.lower()


def _check_config_tiers(domain: str, config: Config) -> int | None:
    """
    Check if domain is in config-defined tiers.
    
    Returns the tier number (1-2 currently) or None if not found.
    """
    for tier_domain in config.domain_tiers.tier_1:
        if tier_domain in domain or domain in tier_domain:
            return 1
    
    for tier_domain in config.domain_tiers.tier_2:
        if tier_domain in domain or domain in tier_domain:
            return 2
    
    return None


async def classify_source(
    domain_or_url: str,
    client: LLMClient,
    config: Config,
) -> int:
    """
    Classify a source domain into credibility tiers (1-5).
    
    Tier definitions:
    - Tier 1: Academic/peer-reviewed (nature.com, arxiv.org)
    - Tier 2: Major news/financial (reuters.com, bloomberg.com)
    - Tier 3: Quality journalism/government sources
    - Tier 4: Blogs, opinion sites
    - Tier 5: Social media, wikis, forums
    
    Args:
        domain_or_url: A domain name or full URL to classify.
        client: Initialized LLMClient for LLM-based classification.
        config: Application config with predefined tier lists.
    
    Returns:
        An integer from 1 (most credible) to 5 (least credible).
    """
    # Extract domain if full URL provided
    if "/" in domain_or_url or ":" in domain_or_url:
        domain = _extract_domain(domain_or_url)
    else:
        domain = domain_or_url.lower()
    
    # Check cache first
    if domain in _domain_tier_cache:
        return _domain_tier_cache[domain]
    
    # Check config-defined tiers
    config_tier = _check_config_tiers(domain, config)
    if config_tier is not None:
        _domain_tier_cache[domain] = config_tier
        return config_tier
    
    # Fall back to LLM classification
    prompt = f"""Classify the domain "{domain}" into a credibility tier for research purposes.

Tier 1: Academic/peer-reviewed sources (journals, .edu, arxiv)
Tier 2: Major news/financial sources (reuters, bloomberg, bbc)
Tier 3: Quality journalism, government, established organizations
Tier 4: Blogs, opinion sites, smaller news outlets
Tier 5: Social media, wikis, forums, user-generated content

Return ONLY a single number from 1 to 5."""

    messages = [{"role": "user", "content": prompt}]
    
    try:
        response = await client.generate_response(messages, temperature=0.1)
        
        # Extract the number from response
        numbers = re.findall(r"\b([1-5])\b", response)
        if numbers:
            tier = int(numbers[0])
        else:
            tier = 4  # Default to lower credibility if unclear
            
    except LLMClientError:
        tier = 4  # Default on error
    
    # Cache the result
    _domain_tier_cache[domain] = tier
    return tier


def clear_tier_cache() -> None:
    """Clear the domain tier cache."""
    _domain_tier_cache.clear()


async def analyze_base_rates(
    client: LLMClient,
    question: str,
    historical_queries: list[str],
    max_results_per_query: int = 5,
) -> str:
    """
    Analyze base rates using search snippets (no full scraping).
    
    Searches for historical precedents and uses LLM to identify
    relevant events and count outcomes from the snippets.
    
    Args:
        client: Initialized LLMClient instance.
        question: The original forecasting question for context.
        historical_queries: List of search queries for historical data.
        max_results_per_query: Max search results per query. Defaults to 5.
    
    Returns:
        A summary text of historical events and their outcomes.
    """
    if not historical_queries:
        return "No historical queries provided for base rate analysis."
    
    # Collect snippets from all queries (no scraping)
    all_snippets: list[dict[str, str]] = []
    
    for query in historical_queries:
        results = await perform_search(query, max_results=max_results_per_query)
        for r in results:
            all_snippets.append({
                "title": r["title"],
                "snippet": r["snippet"],
                "url": r["url"],
            })
    
    if not all_snippets:
        return "No search results found for historical queries."
    
    # Format snippets for LLM
    snippets_text = "\n\n".join(
        f"[{i+1}] {s['title']}\n{s['snippet']}\nSource: {s['url']}"
        for i, s in enumerate(all_snippets)
    )
    
    prompt = f"""You are analyzing historical data to establish base rates for a forecast.

Forecasting Question: {question}

Below are search result snippets from historical queries. Your task:
1. Identify distinct historical events similar to the forecast question.
2. Count the outcomes (how many succeeded vs failed, or similar metrics).
3. Calculate an approximate base rate if possible.
4. Note any patterns or trends.

Search Snippets:
{snippets_text}

Provide a concise summary (2-4 paragraphs) of:
- The historical events you identified
- The outcome counts and any base rate estimate
- Key patterns or caveats

Return only the summary text, no JSON."""

    messages = [{"role": "user", "content": prompt}]
    
    try:
        response = await client.generate_response(messages, temperature=0.3)
        return response.strip()
    except LLMClientError as e:
        return f"Error analyzing base rates: {e}"


async def main() -> None:
    """Test the forecasting logic."""
    config = load_config()
    client = LLMClient(config)
    
    # Test search plan generation
    question = "Will OpenAI release GPT-6 by the end of 2026?"
    print(f"Generating search plan for: {question}")
    print("-" * 50)
    
    plan = await generate_search_plan(client, question, user_ref_class="AI model releases")
    print(f"Search queries: {plan['search_queries']}")
    print(f"Historical queries: {plan['historical_queries']}")
    print(f"Time window: {plan['time_window']}")
    
    print("\n" + "-" * 50)
    print("Testing source classification:")
    
    test_domains = [
        "nature.com",       # Should be Tier 1 (config)
        "reuters.com",      # Should be Tier 2 (config)
        "techcrunch.com",   # Should be classified by LLM
        "reddit.com",       # Should be Tier 5
    ]
    
    for domain in test_domains:
        tier = await classify_source(domain, client, config)
        print(f"  {domain}: Tier {tier}")
    
    # Test base rate analysis
    print("\n" + "-" * 50)
    print("Testing base rate analysis:")
    
    if plan["historical_queries"]:
        base_rate_summary = await analyze_base_rates(
            client,
            question,
            plan["historical_queries"],
            max_results_per_query=3,
        )
        print(f"\nBase Rate Summary:\n{base_rate_summary}")
    else:
        print("No historical queries to analyze.")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
